{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Thesis_for_the_win-3-complete.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["HpDzAdmUDPa3","V1mOf-5jBZ2c","0_RVPrbVBShK","UFWz7BqCC8jx","MqolTa3uBou9","cqQSPk9mLjX4","1YsCUgo8CFem","axHPEiXjL030","sbBE5e1igFhx","KeXbUEsty-xg","heGkEQ-uCb0o","cwuZDuGCCx6D","2ciwqcQZDEqy","F8nVTx0BowPY","fW0-SHrw54E_","YVb3ZA_wjTw-","eOHeGHo6jZOk","UAdjDSygDNwq","-LNPY8lhr80t","CG5xH2NZ2BGu","fRTUFMvzQV3i","1WZi86qgysUL","PB2DiufeJvyd","i7_6QSK1JrTn","kpE9ie499X4a","v52wNTlqQct3","WCQ9B9r5wf1p","db7UY-yCYP_c","SXQuiKKR0yTd"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WB_y_9av0xM-"},"source":["# **General**\n"]},{"cell_type":"markdown","metadata":{"id":"HpDzAdmUDPa3"},"source":["## Mount\n"]},{"cell_type":"code","metadata":{"id":"4BTaL9IS3jTt"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1mOf-5jBZ2c"},"source":["##Import Packages\n"]},{"cell_type":"code","metadata":{"id":"0dgs6Yj3Rb3p"},"source":["%tensorflow_version 1.x\n","from keras.models import Sequential\n","from keras.layers.core import Flatten, Dense, Dropout, Reshape, Lambda\n","from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D, ZeroPadding2D\n","from keras.optimizers import Adam\n","import tensorflow as tf\n","from keras import backend as K\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","import matplotlib.pyplot as plt\n","from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n","import copy\n","import cv2, os\n","import numpy as np\n","from random import shuffle \n","from keras.applications.vgg16 import VGG16 \n","from keras.applications.mobilenet_v2 import MobileNetV2\n","import pickle\n","\n","\n","#From the source - ignore \n","get_ipython().magic(u'matplotlib inline')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqTyGz1IQc8c"},"source":["You may have to run this for using the MobileNetV2 for 3D network's feature extractor on Google Colab. You may also need  to restart session. (not sure)"]},{"cell_type":"code","metadata":{"id":"FukB8g4St8m_"},"source":["pip install 'h5py==2.10.0' --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_RVPrbVBShK"},"source":["##Download the Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"eoS1Nx_6QZiw"},"source":["Download images\n"]},{"cell_type":"code","metadata":{"id":"2b4RJ7GA5q3V"},"source":["!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip\n","!unzip data_object_image_2.zip\n","!ls -l training/image_2 | wc -l  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lT6pIOCuQa7o"},"source":["Download labels\n"]},{"cell_type":"code","metadata":{"id":"73XNYjvJvY2y"},"source":["!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip\n","! unzip data_object_label_2.zip\n","!ls -l training/label_2 | wc -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzgNLXKuQUUO"},"source":["Delete the zip files to save space on google drive\n"]},{"cell_type":"code","metadata":{"id":"68x3KbS4yj_h"},"source":["rm -rf data_object_image_2.zip data_object_label_2.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFWz7BqCC8jx"},"source":["## Setup Tensorboard"]},{"cell_type":"markdown","metadata":{"id":"doSZ0Y10QuhI"},"source":["You may want to change the logdir depending on your training setup\n"]},{"cell_type":"code","metadata":{"id":"fw3EjqcgxGB-"},"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/output_eval\n","\n","# If you want to kill tensorboard session:\n","# !kill [PID]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqolTa3uBou9"},"source":["##Global Parameters"]},{"cell_type":"markdown","metadata":{"id":"cd1ipWwLQ-Hj"},"source":["The Global parameters used for the 3D part\n"]},{"cell_type":"code","metadata":{"id":"wOBCAQFRBtFA"},"source":["# This is for the orientation estimation.\n","BIN, OVERLAP = 6, 0.1\n","\n","W = 1.\n","# The crop shapes for the input 3D network.\n","NORM_H, NORM_W = 224, 224\n","\n","# Maximum bounding box jitter for data augmentation while training 3D network\n","MAX_JIT=5\n","\n","# Classes to be considered and extracted from kitti dataset, while training the 3D network\n","VEHICLES = ['Car', 'Truck', 'Van']\n","\n","# These are used to filter out useless and bad objects from dataset (for training)\n","MAX_OCCLUSION =1 # anything <= will be included.\n","MAX_TRUNCATION = 0.8 # anything < will be included.\n","\n","# Path to the images and labels dir.\n","image_dir = 'training/image_2/'\n","label_dir = 'training/label_2/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGQr8wVWCK-i"},"source":["##Utils (compute_anchors, prepare_input_and_output, data_gen l2_normalize, orientation loss)\n","\n"]},{"cell_type":"code","metadata":{"id":"LDq-xdUaB7yr"},"source":["def compute_anchors(angle):\n","    anchors = []\n","    \n","    wedge = 2.*np.pi/BIN  #Length of each bin in Radian.\n","\n","    # Each angle will lie somewhere in a bin. But we need to find the closest. That is, given a bin, we want to know which bound is closer to our angle.\n","    # For instance if our bin is (pi/2, pi), we want to know whether the angle is easier to reference from pi/2 or pi.\n","    # So we keep both lower and upper bound using l_index and r_index, respectively.\n","    l_index = int(angle/wedge)\n","    r_index = l_index + 1\n","    \n","    # Now we check if our angle is more closer to current bin's lower bound, or its upper bound.\n","\n","    # If close enough to the lower bound, consider current index..\n","    if (angle - l_index*wedge) < wedge/2 * (1+OVERLAP/2):\n","        anchors.append([l_index, angle - l_index*wedge])\n","    # If close enough to upper bound,\n","    if (r_index*wedge - angle) < wedge/2 * (1+OVERLAP/2):\n","        anchors.append([r_index%BIN, angle - r_index*wedge])\n","        \n","    return anchors\n","\n","def prepare_input_and_output(train_inst):\n","\n","    # Read the image.\n","    img = cv2.imread(image_dir + train_inst['image'])\n","\n","    # Read AND jitter each bounding box\n","    xmin = train_inst['xmin'] + np.random.randint(-MAX_JIT, MAX_JIT+1)\n","    ymin = train_inst['ymin'] + np.random.randint(-MAX_JIT, MAX_JIT+1)\n","    xmax = train_inst['xmax'] + np.random.randint(-MAX_JIT, MAX_JIT+1)\n","    ymax = train_inst['ymax'] + np.random.randint(-MAX_JIT, MAX_JIT+1)\n","\n","    # Ensure that your coordinates are withing ranges after jittering.\n","    shape = img.shape\n","    xmin = max(min(xmin, shape[1]-1),0)\n","    xmax = max(min(xmax, shape[1]-1),0)\n","    ymin = max(min(ymin, shape[0]-1),0)\n","    ymax = max(min(ymax, shape[0]-1),0)\n","    \n","    # Crop the frame.\n","    img = copy.deepcopy(img[ymin:ymax+1,xmin:xmax+1]).astype(np.float32)\n","\n","    # Randomly decide whether to flip or not.\n","    flip = np.random.binomial(1, .5)\n","    if flip > 0.5: img = cv2.flip(img, 1)\n","        \n","    # resize the patch to standard size\n","    img = cv2.resize(img, (NORM_H, NORM_W))\n","\n","    # Make pixel values [-0.5 to 0.5].\n","    img = img / 255.0 - 0.5\n","    \n","    #Return data but take care of returining proper orientation (will change if image is flipped).\n","    if flip > 0.5:\n","        return img, train_inst['dims'], train_inst['orient_flipped'], train_inst['conf_flipped']\n","    else:\n","        return img, train_inst['dims'], train_inst['orient'], train_inst['conf']\n","\n","def data_gen(all_objs, batch_size):\n","\n","    # Total objects (patches) that we have\n","    num_obj = len(all_objs)\n","    \n","    keys = range(num_obj)\n","    np.random.shuffle(list(keys))\n","    \n","    # For each batch, we will have indices of [ l_bound ...... r_bound ) (not including r_bound itself).\n","    # Usually r_bound - l_bound  should be equal to batch size.\n","    l_bound = 0\n","    r_bound = batch_size if batch_size < num_obj else num_obj\n","    \n","    while True:\n","        if l_bound == r_bound:\n","            l_bound  = 0\n","            r_bound = batch_size if batch_size < num_obj else num_obj\n","            np.random.shuffle(list(keys))\n","        \n","        currt_inst = 0\n","        x_batch = np.zeros((r_bound - l_bound, 224, 224, 3)) # Image batch\n","        d_batch = np.zeros((r_bound - l_bound, 3)) # Dimension batch\n","        o_batch = np.zeros((r_bound - l_bound, BIN, 2)) # orientation batch\n","        c_batch = np.zeros((r_bound - l_bound, BIN)) # confidences batch\n","        \n","        # Iterate the batch\n","        for key in keys[l_bound:r_bound]:\n","            # Prepare the data for the current frame.\n","            image, dimension, orientation, confidence = prepare_input_and_output(all_objs[key])\n","            \n","            x_batch[currt_inst, :] = image\n","            d_batch[currt_inst, :] = dimension\n","            o_batch[currt_inst, :] = orientation\n","            c_batch[currt_inst, :] = confidence\n","            \n","            currt_inst += 1\n","                \n","        # Yield the prepared batch. \n","        yield x_batch, [d_batch, o_batch, c_batch]\n","\n","        # Go for the next batch\n","        l_bound  = r_bound\n","        r_bound = r_bound + batch_size\n","\n","        # Limit the r_bount to max valid index.\n","        if r_bound > num_obj: r_bound = num_obj \n","\n","def l2_normalize(x):\n","    # Compute the second norm for each (sin, cos) pare and normalize the values.\n","    # So (sin, cos) will be normalized into ( sin/sqrt(sin^2+cos^2) , cos/sqrt(sin^2+cos^2)).\n","    # Thus if the network gives (a,b), we are always sure that a^2 + b^2 = 1 and we can use arctan with no worries.\n","    return tf.nn.l2_normalize(x, axis=2)\n","\n","def orientation_loss(y_true, y_pred):\n","    # Here we have two 3D arrays with shape (batch_size, bin count, 2). the 2 is for the (sin, cos) vector.\n","    # The loss value, however, should be a scalar.\n","    \n","    # Make (sin, cos) into (sin^2, cos^2) and then sum them up into 1 scalar.\n","    # So the shape is now (batch_size, bin_count)\n","    anchors = tf.reduce_sum(tf.square(y_true), axis=2)\n","    # Not sure about this line. I believe it assigns true for every bin.\n","    anchors = tf.greater(anchors, tf.constant(0.5))\n","    # Now for each row, sum the values. So the shape is now (batch size).\n","    anchors = tf.reduce_sum(tf.cast(anchors, tf.float32), 1)\n","    \n","\n","    # We use cosine similarity for the loss. we compute  cos(alpha)= a.b / |a||b|. \n","    # alpha is the angle between ground truth vector and the estimation.\n","    # The ideal value would be cos(0) = 1. But gradient decent tries to MINIMIZE the loss.\n","    # So we add a - behind it. Now the ideal loss value is -1 and the network gets trained in the right direction.\n","    loss = -(y_true[:,:,0]*y_pred[:,:,0] + y_true[:,:,1]*y_pred[:,:,1]) \n","\n","    # For each batch, sum all loss values for all bins. So the shape becomes (batch size)\n","    loss = tf.reduce_sum(loss, axis=1)\n","\n","    # Now normalize the loss. If I've got it right, the anchors are an array of (bin count, bin count, ...)\n","    # So now the loss becomes (loss0/bin_count, loss1/bin_count, ...) with shape (batch size)\n","    loss = loss / anchors\n","    \n","    # Use mean to turn the vector loss into scalar.\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqQSPk9mLjX4"},"source":["##Parse annotation (Function)"]},{"cell_type":"code","metadata":{"id":"LzXgekEMLgoE"},"source":["def parse_annotation(label_dir, image_dir):\n","    # Here we prepare all objects (patches), their attributes, and the\n","    # average dimension across the dataset.\n","    all_objs = []\n","    \n","    # The average vector and the object count used for averaging.\n","    dims_avg = {key:np.array([0, 0, 0]) for key in VEHICLES}\n","    dims_cnt = {key:0 for key in VEHICLES}\n","    \n","    # Iterate through label files.\n","    for label_file in os.listdir(label_dir):\n","\n","        # If you are using another dataset, take care of this.\n","        image_file = label_file.replace('txt', 'png')\n","\n","        # Iterate through lines in each label file\n","        for line in open(label_dir + label_file).readlines():\n","            line = line.strip().split(' ')\n","            # Each row will have this structure:\n","            # Class Truncated Occluded Theta(local) Xmin Ymin Xmax Ymax Dim Dim Dim T T T \n","            truncated = np.abs(float(line[1]))\n","            occluded  = np.abs(float(line[2]))\n","\n","            # Make sure it is a relevant class and with enough visibility (based on global parameters)\n","            if line[0] in VEHICLES and truncated < MAX_TRUNCATION and occluded <= MAX_OCCLUSION:\n","\n","                # This is pretty confusing here. I will explain it somewhere in the documentation. Sorry :(\n","                new_alpha = -float(line[3]) + 3*np.pi/2\n","                new_alpha = new_alpha - np.floor(new_alpha / (2. * np.pi)) * (2. * np.pi)\n","\n","                obj = {'name':line[0],\n","                       'image':image_file,\n","                       'xmin':int(float(line[4])),\n","                       'ymin':int(float(line[5])),\n","                       'xmax':int(float(line[6])),\n","                       'ymax':int(float(line[7])),\n","                       'dims':np.array([float(number) for number in line[8:11]]),\n","                       'new_alpha': new_alpha, \n","                      # The next 3 are not used for training the network, but for benchmarking translation vector accuracy. \n","                       'translation':np.array([float(number) for number in line[11:14]]),\n","                       'truncated': truncated,\n","                       'occluded': occluded\n","                      }\n","                \n","                # Update the average while reading the new object's dimensions\n","                dims_avg[obj['name']]  = dims_cnt[obj['name']]*dims_avg[obj['name']] + obj['dims']\n","                dims_cnt[obj['name']] += 1\n","                dims_avg[obj['name']] /= dims_cnt[obj['name']]\n","\n","                # Add the object to the list\n","                all_objs.append(obj)\n","                \n","    # return the object list and the average dimensions.\n","    return all_objs, dims_avg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YsCUgo8CFem"},"source":["## Process data\n","\n"]},{"cell_type":"code","metadata":{"id":"ZZQ2tOm9U5Uk"},"source":["all_objs, dims_avg = parse_annotation(label_dir, image_dir)\n","\n","print(\"number of objects: {}\".format(len(all_objs)))\n","print(\" ----- \\n A sample object(patch):\\n {} \\n -----\".format(all_objs[0]))\n","print(\"The average dimensions: {}\".format(dims_avg))\n","\n","for obj in all_objs:\n","\n","    # Fix dimensions, compute the residual\n","    obj['dims'] = obj['dims'] - dims_avg[obj['name']]\n","\n","    # Make all residuals 0\n","    orientation = np.zeros((BIN,2))\n","    # Make all bin confidences 0\n","    confidence = np.zeros(BIN)\n","    \n","    #turn angles into -> [bin# , residual]\n","    anchors = compute_anchors(obj['new_alpha'])\n","    \n","    for anchor in anchors:\n","        #compute the cosine and sine of the residual\n","        orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n","\n","        #make the confidence of relative bin# 1\n","        confidence[anchor[0]] = 1.\n","        \n","    #normalize confidences\n","    confidence = confidence / np.sum(confidence)\n","        \n","    #add the computed orientation and confidence to the obj\n","    obj['orient'] = orientation\n","    obj['conf'] = confidence\n","        \n","    # Fix orientation and confidence for flip\n","    orientation = np.zeros((BIN,2))\n","    confidence = np.zeros(BIN)\n","    \n","    anchors = compute_anchors(2.*np.pi - obj['new_alpha'])\n","    \n","    for anchor in anchors:\n","        orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n","        confidence[anchor[0]] = 1\n","        \n","    confidence = confidence / np.sum(confidence)\n","        \n","    obj['orient_flipped'] = orientation\n","    obj['conf_flipped'] = confidence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axHPEiXjL030"},"source":["## Pickle/Unpickle the data"]},{"cell_type":"code","metadata":{"id":"8tkgRfbw__wx"},"source":["#with open('/content/gdrive/MyDrive/objs.pkl', 'wb') as f:\n","#  pickle.dump([all_objs, dims_avg], f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAGZSnyuK6fW"},"source":["\n","# with open('/content/gdrive/MyDrive/objs.pkl', 'rb') as f:\n","#   all_objs, dims_avg = pickle.load(f)\n","# print(len(all_objs), 'objects')\n","# print('sample', all_objs[0])\n","# print('Statistics')\n","# print(dims_avg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbBE5e1igFhx"},"source":["## Load Camera Calibration Files"]},{"cell_type":"code","metadata":{"id":"Pq4Mw7mFgIQu"},"source":["!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip\n","!unzip data_object_calib.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KeXbUEsty-xg"},"source":["## Download and Unzip Pretrained 2D Detection Network"]},{"cell_type":"code","metadata":{"id":"xP8DKLHpu8TQ"},"source":["!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz\n","!tar -xvf faster_rcnn_resnet101_kitti_2018_01_28.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"crdtSN0Fu6eH"},"source":["# **Set up (and Train) 3D Model**"]},{"cell_type":"markdown","metadata":{"id":"heGkEQ-uCb0o"},"source":["## Build 3D Model(Function and usage)"]},{"cell_type":"code","metadata":{"id":"Hu4_ckJAVn4f"},"source":["# Construct the network\n","''' First source code backbone (before Flatten())\n","inputs = Input(shape=(224,224,3))\n","# Block 1\n","x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n","x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","# Block 2\n","x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n","x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n","x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","# Block 3\n","x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n","x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n","x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n","x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","# Block 4\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n","x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","# Block 5\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n","x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n","x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","'''\n","\n","def build_model(input_shape=(224, 224, 3), weights=None, freeze=False):\n","\n","    # feature_extractor_model = VGG16(include_top=False, weights=weights, input_shape=input_shape)\n","    feature_extractor_model = MobileNetV2(include_top=False, weights=weights, input_shape=input_shape)\n","    if freeze:\n","        for layer in feature_extractor_model.layers:\n","            layer.trainable = False\n","\n","    x = Flatten()(feature_extractor_model.output)\n","\n","    dimension   = Dense(512)(x)\n","    dimension   = LeakyReLU(alpha=0.1)(dimension)\n","    dimension   = Dropout(0.5)(dimension)\n","    dimension   = Dense(3)(dimension)\n","    dimension   = LeakyReLU(alpha=0.1, name='dimension')(dimension)\n","\n","    orientation = Dense(256)(x)\n","    orientation = LeakyReLU(alpha=0.1)(orientation)\n","    orientation = Dropout(0.5)(orientation)\n","    orientation = Dense(BIN*2)(orientation)\n","    orientation = LeakyReLU(alpha=0.1)(orientation)\n","    orientation = Reshape((BIN,-1))(orientation)\n","    orientation = Lambda(l2_normalize, name='orientation')(orientation)\n","\n","    confidence  = Dense(256)(x)\n","    confidence  = LeakyReLU(alpha=0.1)(confidence)\n","    confidence  = Dropout(0.5)(confidence)\n","    confidence  = Dense(BIN, activation='softmax', name='confidence')(confidence)\n","\n","    model = Model(feature_extractor_model.input, outputs=[dimension, orientation, confidence])\n","    model.summary() \n","\n","\n","    #load weights\n","    #model.load_weights('/content/gdrive/MyDrive/weights_new2.hdf5')\n","\n","\n","\n","    return model "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwuZDuGCCx6D"},"source":["##Pre-training Stuff"]},{"cell_type":"code","metadata":{"id":"Sls_srHxbFAT"},"source":["model = build_model(input_shape = (224, 224, 3), weights = 'imagenet')\n","\n","early_stop  = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, mode='min', verbose=1)\n","checkpoint  = ModelCheckpoint('/content/gdrive/MyDrive/saves/saved_weights_mobilenet_transition.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n","tensorboard = TensorBoard(log_dir='../logs/', histogram_freq=0, write_graph=True, write_images=False)\n","\n","all_exams  = len(all_objs)\n","trv_split  = int(0.85*all_exams)\n","batch_size = 32\n","# np.random.shuffle(all_objs)\n","\n","train_gen = data_gen(all_objs[:trv_split], batch_size)\n","valid_gen = data_gen(all_objs[trv_split:all_exams], batch_size)\n","\n","train_num = int(np.ceil(trv_split/batch_size))\n","valid_num = int(np.ceil((all_exams - trv_split)/batch_size))\n","\n","print(\"training configurations:\")\n","print(\"all data:\", all_exams)\n","print(\"training data:\", trv_split)\n","print(\"batch size:\", batch_size)\n","print(\"train_num\", train_num)\n","print(\"valid_num\", valid_num)\n","\n","\n","minimizer  = Adam(lr=1e-5)\n","model.compile(optimizer=minimizer, #minimizer,\n","              loss={'dimension': 'mean_squared_error', 'orientation': orientation_loss, 'confidence': 'categorical_crossentropy'},\n","                  loss_weights={'dimension': 2., 'orientation': 1., 'confidence': 4.}, metrics=['mse','mae'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ciwqcQZDEqy"},"source":["## Train 3D Model"]},{"cell_type":"code","metadata":{"id":"6RbwQVwibH3M"},"source":["model.fit_generator(generator = train_gen, \n","                    steps_per_epoch = train_num, \n","                    epochs = 150, \n","                    verbose = 1, \n","                    callbacks = [early_stop, checkpoint, tensorboard],\n","                    validation_data = valid_gen, \n","                    validation_steps = valid_num,\n","#                    workers = 4,\n","#                    use_multiprocessing=True,\n","\n","                    ) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8nVTx0BowPY"},"source":["### Save the model"]},{"cell_type":"code","metadata":{"id":"q-vo6S9dovSl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVB5lHvq2grb"},"source":["# **Test 3D Model's Performance**"]},{"cell_type":"markdown","metadata":{"id":"F1YT0ur16Dy_"},"source":["### Load model Weights"]},{"cell_type":"code","metadata":{"id":"K8RlRcGJyA-N"},"source":["# model.load_weights('/content/gdrive/MyDrive/saves/weights_new2-7.hdf5')\n","# model.load_weights('/content/gdrive/MyDrive/weights_others.h5')\n","#model.load_weights('/content/gdrive/MyDrive/colab/new/saved_weights_mobilenet_new.hdf5')\n","model.load_weights('/content/gdrive/MyDrive/saves/saved_weights_mobilenet_new.hdf5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fW0-SHrw54E_"},"source":["### Open a sample image and patch"]},{"cell_type":"code","metadata":{"id":"PdrjuctU67lk"},"source":["#001024\n","#001100 on train : the github \n","#001112 on train : a car on left towards right\n","#002048 on test : very similar to others' model\n","test_number = '001024'\n","\n","test_image_path = 'testing/image_2/{test_number}.png'.format(test_number = test_number)\n","test_calib_path = 'testing/calib/{test_number}.txt'.format(test_number = test_number)\n","\n","# test_image_path = '/content/0000000005.png'\n","# test_calib_path = '/content/training/calib/000009.txt'\n","# CAUTION::: READING FROM TRAINING FILE!!!!!!!!\n","# test_image_path = 'training/image_2/{test_number}.png'.format(test_number = test_number)\n","# test_calib_path = 'training/calib/{test_number}.txt'.format(test_number = test_number)\n","# test_label_path = 'training/label_2/{test_number}.txt'.format(test_number = test_number)\n","\n","# test_image_path = '0000000011.png'\n","\n","test_image = cv2.imread(test_image_path)\n","test_image = test_image[:, :, ::-1]\n","\n","plt.figure(figsize=(60,60))\n","plt.imshow(test_image/255.)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YVb3ZA_wjTw-"},"source":["### Select BBs by hand (while using test data)\n","\n"]},{"cell_type":"code","metadata":{"id":"MVRzAf1FjR1q"},"source":["#FOR READING FROM TEST FOLDER\n","xmin,ymin,xmax,ymax = (115, 170, 242, 235) #FOR 001024\n","# xmin,ymin,xmax,ymax = (222, 172, 455, 320) #FOR 004701\n","# xmin,ymin,xmax,ymax = (400, 180, 505, 285) #FOR 004999\n","# xmin,xmax,ymin,ymax = (12, 320, 213, 350) #FOR 0000000011.png\n","# xmin,xmax,ymin,ymax = (340, 436, 180, 240) #FOR 002048\n","\n","\n","patch = test_image[ymin:ymax, xmin:xmax]\n","plt.figure()\n","plt.imshow(patch/255.)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOHeGHo6jZOk"},"source":["### Read BBs from label file (while using train data)"]},{"cell_type":"code","metadata":{"id":"Q3EnVLS9679L"},"source":["#CAUTION!!!! FOR READING FROM TRAINING DIRECTORY\n","patches = []\n","bounds = []\n","for line in open(test_label_path).readlines():\n","    line = line.strip().split(' ')\n","    # print(\"2D object: \", line)\n","    xmin=int(float(line[4]))\n","    ymin=int(float(line[5]))\n","    xmax=int(float(line[6]))\n","    ymax=int(float(line[7]))\n","\n","    patch = test_image[ymin:ymax, xmin:xmax]\n","    patch = cv2.resize(patch, (224, 224))\n","    patches.append(patch)\n","    bounds.append((xmin, ymin, xmax, ymax))\n","\n","num_patches = len(patches)\n","fig=plt.figure(figsize=(15, 15))\n","\n","for index, patch in enumerate(patches):\n","    fig.add_subplot(1, num_patches, index+1)\n","    plt.imshow(patch)\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRCbArcQY7iu"},"source":["# CAUTION: FOR TRAINING ONLY: choose one of the above\n","index = 0\n","patch = patches[index]\n","xmin, ymin, xmax, ymax = bounds[index]\n","plt.figure()\n","plt.imshow(patch)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txLuyGG280hd"},"source":["### Read Projection Matrix"]},{"cell_type":"code","metadata":{"id":"q50qUL35BGT4"},"source":["# with open(test_calib_path, \"r\") as f:\n","#     c2c_file = f.readlines()\n","\n","#     for line in c2c_file:\n","#           (key, val) = line.split(':', 1)\n","#           if key == ('P2'):\n","#                 P_ = np.fromstring(val, sep=' ', dtype = 'float')\n","#                 P_ = P_.reshape(3, 4)\n","#                 #print(\"FOUND THIS\", mode, P_)\n","#                 # erase 4th column ([0,0,0])\n","#                 #print(\"THE REMOVING 4TH COLUMN IS:\", P_[:, 3])\n","                \n","#                 calib_mat = P_\n","#                 break\n","print(\"Projection mat:\")\n","calib_mat= [[7.215377e+02, 0.000000e+00, 6.095593e+02, 4.485728e+01],\n","            [0.000000e+00, 7.215377e+02, 1.728540e+02, 2.163791e-01],\n","            [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.745884e-03]]\n","print(calib_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAdjDSygDNwq"},"source":["### Define and Prepare Projection utils"]},{"cell_type":"code","metadata":{"id":"FypLHQZCDNQa"},"source":["def init_points3D(dims):\n","    points3D = np.zeros((8, 3))\n","    cnt = 0\n","    for i in [1, -1]:\n","        for j in [1, -1]:\n","            for k in [1, -1]:\n","                points3D[cnt] = dims[[1, 0, 2]].T / 2.0 * [i, k, j * i]\n","                cnt += 1\n","    return points3D\n","\n","def gen_3D_box(yaw,dims,cam_to_img,box_2D):\n","    \n","    dims = dims.reshape((-1,1))\n","    box_2D = box_2D.reshape((-1,1))\n","    points3D = init_points3D(dims)\n","\n","    rot_M = np.asarray([[np.cos(yaw), 0, np.sin(yaw)], [0, 1, 0], [-np.sin(yaw), 0, np.cos(yaw)]]) \n","    #rot_M =  np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) \n","    center = compute_center(points3D, rot_M, cam_to_img, box_2D, inds)\n","\n","    points2D = points3D_to_2D(points3D, center, rot_M, cam_to_img)\n","\n","    return points2D\n","\n","def compute_center(points3D,rot_M,cam_to_img,box_2D,inds):\n","    \n","    fx = cam_to_img[0][0]\n","    fy = cam_to_img[1][1]\n","    u0 = cam_to_img[0][2]\n","    v0 = cam_to_img[1][2]\n","    \n","    W = np.array([[fx, 0, u0 - box_2D[0]],\n","                  [fx, 0, u0 - box_2D[2]],\n","                  [0, fy, v0 - box_2D[1]],\n","                  [0, fy, v0 - box_2D[3]]], dtype = 'float')\n","    center =None\n","    error_min = 1e10\n","\n","    for ind in inds:\n","        y = np.zeros((4, 1))\n","        for i in range(len(ind)):\n","            \n","            RP = np.dot(rot_M, (points3D[ind[i]]).reshape((-1, 1)))\n","            y[i] = box_2D[i] * cam_to_img[2, 3] - np.dot(W[i], RP) - cam_to_img[i // 2, 3]\n","            \n","        result = solve_least_squre(W, y)\n","        error = compute_error(points3D, result, rot_M, cam_to_img, box_2D)\n","        \n","        if error < error_min and result[2,0]>0:\n","            center = result\n","            error_min = error\n","            \n","    return center\n","\n","def draw_3D_box(image,points):\n","    points = points.astype(np.int)\n","\n","    for i in range(4):\n","        point_1_ = points[2 * i]\n","        point_2_ = points[2 * i + 1]\n","        cv2.line(image, (point_1_[0], point_1_[1]), (point_2_[0], point_2_[1]), (0, 255, 0), 1)\n","\n","    cv2.line(image,tuple(points[0]),tuple(points[7]),(0, 0, 255), 2)\n","    cv2.line(image, tuple(points[1]), tuple(points[6]), (0, 0, 255), 2)\n","\n","    for i in range(8):\n","        point_1_ = points[i]\n","        point_2_ = points[(i + 2) % 8]\n","        cv2.line(image, (point_1_[0], point_1_[1]), (point_2_[0], point_2_[1]), (0, 255, 0), 1)\n","\n","    return image;\n","\n","def solve_least_squre(W,y):\n","    U, Sigma, VT = np.linalg.svd(W)\n","    result = np.dot(np.dot(np.dot(VT.T, np.linalg.pinv(np.eye(4, 3) * Sigma)), U.T), y)\n","    return result\n","\n","def points3D_to_2D(points3D,center,rot_M,cam_to_img):\n","    points2D = []\n","    for point3D in points3D:\n","        point3D = point3D.reshape((-1,1))\n","        point = center + np.dot(rot_M, point3D)\n","        point = np.append(point, 1)\n","        point = np.dot(cam_to_img, point)\n","        point2D = point[:2] / point[2]\n","        points2D.append(point2D)\n","    points2D = np.asarray(points2D)\n","\n","    return points2D\n","\n","def compute_error(points3D,center,rot_M, cam_to_img,box_2D):\n","    points2D = points3D_to_2D(points3D, center, rot_M, cam_to_img)\n","    new_box_2D = np.asarray([np.min(points2D[:,0]),\n","                             np.max(points2D[:,0]),\n","                             np.min(points2D[:,1]),\n","                             np.max(points2D[:,1])]).reshape((-1,1))\n","    error = np.sum(np.abs(new_box_2D - box_2D))\n","\n","    return error\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIhbCvHz-DR9"},"source":["### Project and Draw the 3D box"]},{"cell_type":"code","metadata":{"id":"FM4HYpOQDkJ_"},"source":["inds = []\n","indx = [1, 3, 5, 7]\n","indy = [0, 1, 2, 3]\n","for i in indx:\n","    for j in indx:\n","        for m in indy:\n","            for n in indy:\n","                inds.append([i, j, m, n])\n","\n","box_2D = np.asarray([xmin, ymin, xmax, ymax], dtype = np.float)\n","points2D = gen_3D_box(yaw, dims, calib_mat, box_2D) #switched yaw -> theta    \n","final_image = draw_3D_box(test_image.copy(), points2D)\n","\n","plt.figure(figsize=(20,20))\n","plt.imshow(final_image/255.)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1L8SAKniyGwj"},"source":["# Test translation vector accuracy\n"]},{"cell_type":"code","metadata":{"id":"tETUPK7iyMMM"},"source":["def get_t(yaw,dims,cam_to_img,box_2D, ind):\n","    dims = dims.reshape((-1,1))\n","    box_2D = box_2D.reshape((-1,1))\n","    points3D = init_points3D(dims)\n","    rot_M = np.asarray([[np.cos(yaw), 0, np.sin(yaw)], [0, 1, 0], [-np.sin(yaw), 0, np.cos(yaw)]]) \n","    #rot_M =  np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) \n","    center = compute_center(points3D, rot_M, cam_to_img, box_2D, inds)\n","    # print(\"Center is: \", center)\n","    return center\n","\n","def compare_t(estimated_t, true_t):\n","\n","  estimated = np.asarray([float(estimated_t[0]), float(estimated_t[1]), float(estimated_t[2])])\n","  # print(\"Estimated \", estimated)\n","  # print(\"Actual \", true_t)\n","  dist = np.linalg.norm(true_t - estimated)\n","  normalized_dist = np.linalg.norm(true_t - estimated) / np.linalg.norm(true_t)\n","  # print(dist)\n","  # print(dist, normalized_dist)\n","  return dist, normalized_dist\n","\n","avg_t_loss=0\n","avg_nt_loss=0\n","cnt = 0\n","for obj in all_objs[trv_split:all_exams]:\n","# for obj in all_objs[trv_split:trv_split+10]:\n","    truncated = obj['truncated']\n","    occluded = obj['occluded']\n","    height = int(obj['ymax'] - obj['ymin'])\n","    width = int(obj['xmax'] - obj['xmin'])\n","    if (truncated > 0.5 or occluded> 1 or height <=25 or width <=25):\n","      continue\n","\n","    with open('/content/training/calib/'+obj['image'].split('.')[0]+'.txt', \"r\") as f:\n","      c2c_file = f.readlines()\n","\n","    for line in c2c_file:\n","          (key, val) = line.split(':', 1)\n","          if key == ('P2'):\n","                P_ = np.fromstring(val, sep=' ', dtype = 'float')\n","                P_ = P_.reshape(3, 4)\n","                calib_mat = P_\n","                break\n","    # print(obj['truncated'], obj['occluded'])\n","    xmin = int(obj['xmin'])\n","    ymin = int(obj['ymin'])\n","    xmax = int(obj['xmax'])\n","    ymax = int(obj['ymax'])\n","\n","    test_image = cv2.imread('/content/training/image_2/'+obj['image'])\n","    patch = test_image[ymin:ymax, xmin:xmax]\n","    patch = cv2.resize(patch, (224, 224))\n","    patch = patch / 255.0 - 0.5\n","\n","    patch = np.expand_dims(patch, axis = 0)\n","\n","    prediction = model.predict(patch)\n","    BIN = 6\n","    max_anc = np.argmax(prediction[2][0])\n","    anchors = prediction[1][0][max_anc]\n","\n","    if anchors[1] > 0:\n","        angle_offset = np.arccos(anchors[0])\n","    else:\n","        angle_offset = -np.arccos(anchors[0])\n","        \n","    wedge = 2.*np.pi/BIN\n","    theta_loc = angle_offset + max_anc*wedge\n","\n","    fx = calib_mat[0][0]\n","    u0 = calib_mat[0][2]\n","    v0 = calib_mat[1][2]\n","\n","    box2d_center_x= (xmin + xmax) / 2.0\n","    \n","    theta_ray = np.arctan(fx /(box2d_center_x - u0))\n","    if theta_ray<0:\n","          theta_ray = theta_ray+np.pi\n","\n","    # Theta Yaw\n","    theta = theta_loc + theta_ray\n","    yaw = np.pi/2 - theta\n","\n","    #DIMENSION\n","    dims_avg = {'Car' : [1.52130159, 1.64441129, 3.85729945],\n","                'Truck': [ 3.07044968,  2.62877944, 11.17126338],\n","                'Van': [2.18560847, 1.91077601, 5.08042328],\n","                'Tram': [3.56005102,  2.4002551,  18.52173469] }\n","    dims = dims_avg[obj['name']] + prediction[0][0]\n","\n","    inds = []\n","    indx = [1, 3, 5, 7]\n","    indy = [0, 1, 2, 3]\n","    for i in indx:\n","        for j in indx:\n","            for m in indy:\n","                for n in indy:\n","                    inds.append([i, j, m, n])\n","\n","    box_2D = np.asarray([xmin, ymin, xmax, ymax], dtype = np.float)\n","    points2D = gen_3D_box(yaw, dims, calib_mat, box_2D) #switched yaw -> theta    \n","    final_image = draw_3D_box(test_image, points2D)\n","\n","    # final_image = final_image[:, :, ::-1]\n","    # plt.figure(figsize=(60,60))\n","    # plt.imshow(final_image/255.)\n","    # plt.show()\n","  \n","    t = get_t(yaw, dims, calib_mat, box_2D, inds) #switched yaw -> theta    \n","    t_loss, nt_loss = compare_t(t, obj['translation'])\n","    cnt += 1\n","    avg_t_loss = ( (avg_t_loss * (cnt-1)) + t_loss )/cnt\n","    avg_nt_loss = ( (avg_nt_loss * (cnt-1)) + nt_loss )/cnt\n","    \n","print(\"Average loss is: \", avg_t_loss)\n","print(\"Average normalized loss is: \", avg_nt_loss)\n","print(\"Count: \", cnt)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-LNPY8lhr80t"},"source":["# **Set Up and Test 2D Model** "]},{"cell_type":"markdown","metadata":{"id":"kflU9r96iYK8"},"source":["## Setup Object Detection API"]},{"cell_type":"code","metadata":{"id":"D__J6hW4idoy"},"source":["!git clone https://github.com/tensorflow/models.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycqfFms7dGgh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps79d6aGqnPE"},"source":["!pip install protobuf protobuf-compiler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yml8cy8di9fy"},"source":["%cd models/research\n","\n","# Compile protos.\n","!protoc object_detection/protos/*.proto --python_out=.\n","# Install TensorFlow Object Detection API.\n","%cp object_detection/packages/tf1/setup.py .\n","!python -m pip install ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAqRMPvNjHL7"},"source":["!python object_detection/builders/model_builder_tf1_test.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-w5lVl4yswN0"},"source":["%cd ../.."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7B63PKr1s8BO"},"source":["## Download 2D detection model"]},{"cell_type":"code","metadata":{"id":"qkasNVivs7S5"},"source":["!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz\n","!tar -xvf faster_rcnn_resnet101_kitti_2018_01_28.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b0HZasSwAvL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CG5xH2NZ2BGu"},"source":["## Load and Test 2D Model"]},{"cell_type":"code","metadata":{"id":"HUnd07991__u"},"source":["def load_graph(frozen_graph_filename):\n","    # We load the protobuf file from the disk and parse it to retrieve the \n","    # unserialized graph_def\n","    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n","        graph_def = tf.GraphDef()\n","        graph_def.ParseFromString(f.read())\n","\n","    # yThen, we import the graph_def into a new Graph and returns it \n","    with tf.Graph().as_default() as graph:\n","        # The name var will prefix every op/nodes in your graph\n","        # Since we load everthing in a new graph, this is not needed\n","        tf.import_graph_def(graph_def, name=\"2D\")\n","    return graph\n","\n","graph = load_graph('/content/faster_rcnn_resnet101_kitti_2018_01_28/frozen_inference_graph.pb')\n","\n","# We can verify that we can access the list of operations in the graph\n","\n","# for op in graph.get_operations():\n","#    print(op.name)\n","\n","    # prefix/Placeholder/inputs_placeholder\n","    # ...\n","    # prefix/Accuracy/predictions\n","\n","\n","\n","# We access the input and output nodes \n","image_tensor = graph.get_tensor_by_name('2D/image_tensor:0')\n","# print(tf.shape(image_tensor))\n","scores_tensor = graph.get_tensor_by_name('2D/detection_scores:0')\n","# print(tf.shape(scores_tensor))\n","boxes_tensor = graph.get_tensor_by_name('2D/detection_boxes:0')\n","# print(tf.shape(boxes_tensor))\n","classes_tensor = graph.get_tensor_by_name('2D/detection_classes:0')\n","# print(tf.shape(classes_tensor))\n","num_tensor = graph.get_tensor_by_name('2D/num_detections:0')\n","# print(tf.shape(num_tensor))\n","\n","    \n","# We launch a Session\n","\n","\n","test_image = cv2.imread(test_image_path)\n","image_width = test_image.shape[1]\n","image_height = test_image.shape[0]\n","# print(test_image)\n","with tf.Session(graph=graph) as sess:\n","\n","    \n","    (boxes, scores, classes, num) = sess.run(\n","        [boxes_tensor, scores_tensor, classes_tensor, num_tensor],\n","        feed_dict={image_tensor: [test_image]})\n","# print(boxes)\n","# print(boxes.shape)\n","# print(scores)\n","# print(classes)\n","# print(num)\n","\n","show= test_image.copy()\n","real_boxes=[]\n","for index in range(int(num[0])):\n","  box = boxes[0][index]\n","  real_box = (int(box[0]*image_height), int(box[1]*image_width), int(box[2]*image_height), int(box[3]*image_width))\n","  print(real_box, scores[0][index])\n","  show = cv2.rectangle(show, (real_box[1], real_box[0]), (real_box[3], real_box[2]), color=(0,0,255), thickness=1)\n","  real_boxes.append(real_box)\n","\n","show = show[:, :, ::-1]\n","\n","plt.figure(figsize=(60,60))\n","plt.imshow(show/255.)\n","plt.show()\n","    \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRTUFMvzQV3i"},"source":["## 2D to 3D (For Testing 2D Model's boxes)\n"]},{"cell_type":"code","metadata":{"id":"Fqqkqna6QXwk"},"source":["final_image = test_image.copy()\n","for index, box in enumerate(real_boxes):\n","    ymin, xmin, ymax, xmax = box\n","    patch = test_image[ymin:ymax, xmin:xmax]\n","    patch = cv2.resize(patch, (224, 224))\n","\n","    patch = patch / 255.0 - 0.5\n","    patch = np.expand_dims(patch, axis = 0)\n","    prediction = model.predict(patch)\n","\n","    BIN = 6\n","    max_anc = np.argmax(prediction[2][0])\n","    anchors = prediction[1][0][max_anc]\n","\n","    if anchors[1] > 0:\n","        angle_offset = np.arccos(anchors[0])\n","    else:\n","        angle_offset = -np.arccos(anchors[0])\n","    wedge = 2.*np.pi/BIN\n","    theta_loc = angle_offset + max_anc*wedge\n","\n","    fx = calib_mat[0][0]\n","    u0 = calib_mat[0][2]\n","    v0 = calib_mat[1][2]\n","\n","    box2d_center_x= (xmin + xmax) / 2.0\n","    theta_ray = np.arctan(fx /(box2d_center_x - u0))\n","\n","    if theta_ray<0:\n","          theta_ray = theta_ray+np.pi\n","\n","    # Theta Yaw\n","    theta = theta_loc + theta_ray\n","    yaw = np.pi/2 - theta\n","\n","    #DIMENSION\n","    dims_avg = {'Car' : [1.52130159, 1.64441129, 3.85729945],\n","                'Truck': [ 3.07044968,  2.62877944, 11.17126338],\n","                'Van': [2.18560847, 1.91077601, 5.08042328],\n","                'Tram': [3.56005102,  2.4002551,  18.52173469] }\n","    dims = dims_avg['Car'] + prediction[0][0]\n","\n","    inds = []\n","    indx = [1, 3, 5, 7]\n","    indy = [0, 1, 2, 3]\n","    for i in indx:\n","        for j in indx:\n","            for m in indy:\n","                for n in indy:\n","                    inds.append([i, j, m, n])\n","\n","    box_2D = np.asarray([box[1], box[0], box[3], box[2]], dtype = np.float)\n","    points2D = gen_3D_box(yaw, dims, calib_mat, box_2D) #switched yaw -> theta    \n","    final_image = draw_3D_box(final_image, points2D)\n","\n","final_image = final_image[:, :, ::-1]\n","plt.figure(figsize=(20,20))\n","plt.imshow(final_image/255.)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Q3Znq7b0m0J"},"source":["# **Complete Test**"]},{"cell_type":"markdown","metadata":{"id":"cMuxSZMmtiBo"},"source":["## Open a Sample image from test/train dataset"]},{"cell_type":"code","metadata":{"id":"4nJAwem8tg0y"},"source":["#001024\n","#001100 on train : the github \n","#001112 on train : a car on left towards right\n","#002048 on test : very similar to others' model\n","\n","# 002950 is 1\n","# 000517 from paper\n","# 940 not bad\n","# 993 maybe\n","# 996 maybe\n","test_number = '000106'\n","\n","\n","# 000075 1 thesis\n","\n","test_image_path = 'testing/image_2/{test_number}.png'.format(test_number = test_number)\n","test_calib_path = 'testing/calib/{test_number}.txt'.format(test_number = test_number)\n","\n","# CAUTION::: READING FROM TRAINING FILE!!!!!!!!\n","# test_image_path = 'training/image_2/{test_number}.png'.format(test_number = test_number)\n","# test_calib_path = 'training/calib/{test_number}.txt'.format(test_number = test_number)\n","# test_label_path = 'training/label_2/{test_number}.txt'.format(test_number = test_number)\n","\n","# test_image_path = '0000000011.png'\n","\n","test_image = cv2.imread(test_image_path)\n","test_image = test_image[:, :, ::-1]\n","\n","plt.figure(figsize=(60,60))\n","plt.imshow(test_image/255.)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WFOFxI2Mafn"},"source":["with open(test_calib_path, \"r\") as f:\n","    c2c_file = f.readlines()\n","\n","    for line in c2c_file:\n","          (key, val) = line.split(':', 1)\n","          if key == ('P2'):\n","                P_ = np.fromstring(val, sep=' ', dtype = 'float')\n","                P_ = P_.reshape(3, 4)\n","                #print(\"FOUND THIS\", mode, P_)\n","                # erase 4th column ([0,0,0])\n","                #print(\"THE REMOVING 4TH COLUMN IS:\", P_[:, 3])\n","                \n","                calib_mat = P_\n","                break\n","print(\"Projection mat:\")\n","# calib_mat= [[7.215377e+02, 0.000000e+00, 6.095593e+02, 4.485728e+01],\n","#             [0.000000e+00, 7.215377e+02, 1.728540e+02, 2.163791e-01],\n","#             [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.745884e-03]]\n","print(calib_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yw109rUuttoE"},"source":["## Setup pre-trained 2D Model and Detect Vehicles in 2D"]},{"cell_type":"code","metadata":{"id":"jH_U8R1DtzJQ"},"source":["def load_graph(frozen_graph_filename):\n","    # We load the protobuf file from the disk and parse it to retrieve the \n","    # unserialized graph_def\n","    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n","        graph_def = tf.GraphDef()\n","        graph_def.ParseFromString(f.read())\n","\n","    # yThen, we import the graph_def into a new Graph and returns it \n","    with tf.Graph().as_default() as graph:\n","        # The name var will prefix every op/nodes in your graph\n","        # Since we load everthing in a new graph, this is not needed\n","        tf.import_graph_def(graph_def, name=\"2D\")\n","    return graph\n","\n","graph = load_graph('/content/gdrive/MyDrive/saves/2D/frozen_inference_graph_25000.pb')\n","\n","# We can verify that we can access the list of operations in the graph\n","\n","# for op in graph.get_operations():\n","#    print(op.name)\n","\n","    # prefix/Placeholder/inputs_placeholder\n","    # ...\n","    # prefix/Accuracy/predictions\n","\n","\n","\n","# We access the input and output nodes \n","image_tensor = graph.get_tensor_by_name('2D/image_tensor:0')\n","# print(tf.shape(image_tensor))\n","scores_tensor = graph.get_tensor_by_name('2D/detection_scores:0')\n","# print(tf.shape(scores_tensor))\n","boxes_tensor = graph.get_tensor_by_name('2D/detection_boxes:0')\n","# print(tf.shape(boxes_tensor))\n","classes_tensor = graph.get_tensor_by_name('2D/detection_classes:0')\n","# print(tf.shape(classes_tensor))\n","num_tensor = graph.get_tensor_by_name('2D/num_detections:0')\n","# print(tf.shape(num_tensor))\n","\n","    \n","# We launch a Session\n","\n","\n","test_image = cv2.imread(test_image_path)\n","image_width = test_image.shape[1]\n","image_height = test_image.shape[0]\n","# print(test_image)\n","with tf.Session(graph=graph) as sess:\n","\n","    \n","    (boxes, scores, classes, num) = sess.run(\n","        [boxes_tensor, scores_tensor, classes_tensor, num_tensor],\n","        feed_dict={image_tensor: [test_image]})\n","# print(boxes)\n","# print(boxes.shape)\n","# print(scores)\n","# print(classes)\n","# print(num)\n","\n","show= test_image.copy()\n","real_boxes=[]\n","real_classes = []\n","for index in range(int(num[0])):\n","  if (scores[0][index] < 0.75):\n","    continue\n","  box = boxes[0][index]\n","  real_box = (int(box[0]*image_height), int(box[1]*image_width), int(box[2]*image_height), int(box[3]*image_width))\n","  print(real_box, scores[0][index],classes[0][index])\n","  show = cv2.rectangle(show, (real_box[1], real_box[0]), (real_box[3], real_box[2]), color=(0,0,255), thickness=2)\n","  real_boxes.append(real_box)\n","  real_classes.append(int(classes[0][index]))\n","show = show[:, :, ::-1]\n","\n","plt.figure(figsize=(60,60))\n","plt.imshow(show/255.)\n","plt.show()\n","    \n","\n","print(real_classes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4iTS2fAuVya"},"source":["## 2D to 3D"]},{"cell_type":"code","metadata":{"id":"Vzfkzlb7uatf"},"source":["final_image = test_image.copy()\n","for index, box in enumerate(real_boxes):\n","    print(real_classes[index])\n","    ymin, xmin, ymax, xmax = box\n","    patch = test_image[ymin:ymax, xmin:xmax]\n","    patch = cv2.resize(patch, (224, 224))\n","\n","    patch = patch / 255.0 - 0.5\n","    patch = np.expand_dims(patch, axis = 0)\n","    prediction = model.predict(patch)\n","\n","    BIN = 6\n","    max_anc = np.argmax(prediction[2][0])\n","    anchors = prediction[1][0][max_anc]\n","\n","    if anchors[1] > 0:\n","        angle_offset = np.arccos(anchors[0])\n","    else:\n","        angle_offset = -np.arccos(anchors[0])\n","    wedge = 2.*np.pi/BIN\n","    theta_loc = angle_offset + max_anc*wedge\n","\n","    fx = calib_mat[0][0]\n","    u0 = calib_mat[0][2]\n","    v0 = calib_mat[1][2]\n","\n","    box2d_center_x= (xmin + xmax) / 2.0\n","    \n","    theta_ray = np.arctan(fx /(box2d_center_x - u0))\n","    if theta_ray<0:\n","          theta_ray = theta_ray+np.pi\n","\n","    # Theta Yaw\n","    theta = theta_loc + theta_ray\n","    yaw = np.pi/2 - theta\n","\n","    #DIMENSION\n","    dims_avg = {'Car' : [1.52130159, 1.64441129, 3.85729945],\n","                'Truck': [ 3.07044968,  2.62877944, 11.17126338],\n","                'Van': [2.18560847, 1.91077601, 5.08042328],\n","                'Tram': [3.56005102,  2.4002551,  18.52173469] }\n","\n","    label_map = ['Car', 'Pedestrian','Van', 'Truck']\n","    dims = dims_avg[label_map[real_classes[index] - 1]] + prediction[0][0]\n","    print(label_map[real_classes[index] - 1])\n","\n","    inds = []\n","    indx = [1, 3, 5, 7]\n","    indy = [0, 1, 2, 3]\n","    for i in indx:\n","        for j in indx:\n","            for m in indy:\n","                for n in indy:\n","                    inds.append([i, j, m, n])\n","\n","    box_2D = np.asarray([box[1], box[0], box[3], box[2]], dtype = np.float)\n","    points2D = gen_3D_box(yaw, dims, calib_mat, box_2D) #switched yaw -> theta    \n","    final_image = draw_3D_box(final_image, points2D)\n","\n","final_image = final_image[:, :, ::-1]\n","plt.figure(figsize=(60,60))\n","plt.imshow(final_image/255.)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WZi86qgysUL"},"source":["# Train 2D Network Yourself"]},{"cell_type":"markdown","metadata":{"id":"PB2DiufeJvyd"},"source":["## Get FRCNN_Resnet_101"]},{"cell_type":"code","metadata":{"id":"zyRzOBr4JvF7"},"source":["%cd /content\n","!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz\n","!tar -xvf faster_rcnn_resnet101_kitti_2018_01_28.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7_6QSK1JrTn"},"source":["## Get Mobilenet_V2+SSD\n"]},{"cell_type":"code","metadata":{"id":"oerbiRMOJq8w"},"source":["%cd /content\n","!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n","!tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kpE9ie499X4a"},"source":["## Generate TF record"]},{"cell_type":"code","metadata":{"id":"tA2uzl7D-vLm"},"source":["# In case you are running this cell again:\n","%cd /content\n","!rm -rf data\n","!rm -rf ouptut\n","!rm -rf data_object_image_2\n","\n","\n","\n","%mkdir data_object_image_2\n","%cd data_object_image_2\n","%mkdir training\n","%cd ..\n","%cp -r training/image_2 data_object_image_2/training/\n","%mkdir data\n","!mkdir output\n","!touch data/kitti_label_map.pbtxt\n","!printf 'item {\\n id: 1\\n name: \"car\"\\n}\\nitem {\\n id: 2\\n name: \"pedestrian\"\\n}\\nitem {\\n id: 3\\n name: \"van\"\\n}\\nitem {\\n id: 4\\n name: \"truck\"\\n}' >> data/kitti_label_map.pbtxt\n","print(\"MAKE SURE TO FIX THE LABELMAP FILES' \\n S\" )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYgh2rUqCXqW"},"source":["!rm -rf /content/data_object_image_2/training/image_2/.ipynb_checkpoints\n","%cp models/research/object_detection/legacy/train.py models/research/object_detection\n","%cp /content/models/research/object_detection/legacy/eval.py /content/models/research/object_detection"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAuRhA6I9aPg"},"source":["!python /content/models/research/object_detection/dataset_tools/create_kitti_tf_record.py \\\n","--data_dir=/content \\\n","--output_path=/content/output/kitti.record \\\n","--classes_to_use=car,pedestrian,van,truck \\\n","--validation_set_side=500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v52wNTlqQct3"},"source":["## Train the model\n"]},{"cell_type":"code","metadata":{"id":"SiM_OpCi5CZ7"},"source":["# %cp output/kitti.record* gdrive/MyDrive\n","# %cp /content/faster_rcnn_resnet101_kitti_2018_01_28/pipeline.config gdrive/MyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sSJcU_FGS6T"},"source":["%cd /content\n","# !rm -rf /content/output_train\n","\n","!python models/research/object_detection/train.py \\\n","--logdir=/content/log \\\n","--train_dir=/content/output_train \\\n","--pipeline_config_path=/content/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvrFTwkmv6Qq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCQ9B9r5wf1p"},"source":["## Convert checkpoint to pb"]},{"cell_type":"code","metadata":{"id":"1ecfkkYBwlCj"},"source":["\n","# # trained_checkpoint_prefix = '/content/gdrive/MyDrive/output_train/model.ckpt-3228'\n","\n","# trained_checkpoint_prefix = '/content/gdrive/MyDrive/colab/1000/frcnn/model.ckpt-1000'\n","# export_dir = '/content/gdrive/MyDrive/colab/1000/frcnn/3'\n","\n","# graph = tf.Graph()\n","# with tf.compat.v1.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n","#     # Restore from checkpoint\n","#     loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\n","#     loader.restore(sess, trained_checkpoint_prefix)\n","    \n","\n","#     # Export checkpoint to SavedModel\n","#     builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\n","    \n","#     builder.add_meta_graph_and_variables(sess,\n","#                                          [tf.saved_model.SERVING],\n","#                                          strip_default_attrs=True)\n","    \n","#     builder.save()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"db7UY-yCYP_c"},"source":["## Convert checkpoint to frozen graph\n"]},{"cell_type":"code","metadata":{"id":"Bh9pYVn9YTvf"},"source":["# meta_path = '/content/output_train/model.ckpt-714.meta' # Your .meta file\n","# output_node_names = ['output:0']    # Output nodes    \n","\n","# with tf.Session() as sess:\n","#     # Restore the graph\n","#     saver = tf.train.import_meta_graph(meta_path)\n","\n","#     # Load weights\n","#     saver.restore(sess,tf.train.latest_checkpoint('/content/output_train/checkpoint'))\n","\n","#     output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]\n","#     print(output_node_names)\n","#     exit()\n","#     # Freeze the graph\n","#     frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n","#         sess,\n","#         sess.graph_def,\n","#         output_node_names)\n","\n","#     # Save the frozen graph\n","#     with open('output_graph.pb', 'wb') as f:\n","#       f.write(frozen_graph_def.SerializeToString())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4w1nsIvfQv1"},"source":["# def frozen_graph_maker(export_dir,output_graph):\n","#     with tf.Session(graph=tf.Graph(), config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n","#       tf.saved_model.loader.load(sess, [tf.saved_model.SERVING], export_dir)\n","#       nodes = [n for n in tf.get_default_graph().as_graph_def().node]\n","#       ops = sess.graph.get_operations()\n","#       for op in ops:\n","#         if op.name.find(':0') != -1: \n","#           print(op.name)\n","#       return\n","#       #print(nodes[0].extensions)\n","#       #exit(-1)\n","#       # for n in tf.get_default_graph().as_graph_def().node:\n","#       #  print(n.name)\n","#       #  print(dir(n))\n","#       #  exit(-1)\n","#       # exit(-1)\n","#       output_graph_def = tf.graph_util.convert_variables_to_constants(\n","#               sess, # The session is used to retrieve the weights\n","#               sess.graph_def,\n","#               output_nodes# The output node names are used to select the usefull nodes\n","#       )       \n","#     # Finally we serialize and dump the output graph to the filesystem\n","#     with tf.gfile.GFile(output_graph, \"wb\") as f:\n","#             f.write(output_graph_def.SerializeToString())\n","\n","# export_dir='/content/gdrive/MyDrive/colab/1000/frcnn/2' \n","# output_graph = \"frozen_graph.pb\"\n","# frozen_graph_maker(export_dir,output_graph)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnNtcgO-85rD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gd2rv2AE9Ia3"},"source":["%rm -rf /content/output_eval\n","%cd /content/models/research\n","!python object_detection/eval.py \\\n","        --logtostderr \\\n","        --checkpoint_dir=/content/output_train \\\n","        --eval_dir=/content/output_eval \\\n","        --pipeline_config_path=/content/output_train/pipeline.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBPrq6qsAlh6"},"source":["%cd /content/models/research/\n","\n","!python object_detection/export_inference_graph.py \\\n","--input_type=image_tensor \\\n","--pipeline_config_path=/content/gdrive/MyDrive/colab/new/25_May/output_train/pipeline.config \\\n","--trained_checkpoint_prefix=/content/gdrive/MyDrive/colab/new/25_May/output_train/model.ckpt-25000 \\\n","--output_directory=/content/gdrive/MyDrive/colab/new/25_May/Frozen\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHoFZsEwSoPN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXQuiKKR0yTd"},"source":["## pare_annotation_2d"]},{"cell_type":"code","metadata":{"id":"Zp4icTDQr--j"},"source":["def parse_annotation_2d(label_dir, image_dir):\n","\n","    print(len(os.listdir(label_dir)), \"files found\")\n","\n","    labels = []\n","    images = []\n","\n","    for label_file in os.listdir(label_dir):\n","\n","        image_file = label_file.replace('txt', 'png')\n","\n","        instance_annotations = []\n","        \n","        for line in open(label_dir + label_file).readlines():\n","          \n","            line = line.strip().split(' ')\n","            truncated = np.abs(float(line[1]))\n","            occluded  = np.abs(float(line[2]))\n","            \n","            if line[0] in VEHICLES and truncated < 0.3 and occluded <= 1:\n","                \n","                category = line[0]\n","                xmin = int(float(line[4]))\n","                ymin = int(float(line[5]))\n","                xmax = int(float(line[6]))\n","                ymax = int(float(line[7]))\n","                \n","                instance_annotations.append((xmin, ymin, xmax, ymax, category))\n","\n","        \n","        # img = cv2.imread(image_dir + image_file)\n","\n","        images.append(image_dir + image_file)\n","        labels.append(np.array(instance_annotations))\n","  \n","    labels = np.array(labels)\n","    images = np.array(images)\n","\n","    return images, labels\n","\n","\n","\"\"\" Unit test\n","\n","index = 5\n","images, labels = parse_annotation_2d(label_dir, image_dir)\n","print(\"images\", images.shape)\n","print(\"labels\", labels.shape)\n","\n","img = cv2.imread(images[index])[:, :, ::-1]\n","print(img.shape)\n","plt.figure(figsize=(60,60))\n","plt.imshow(img/255.)\n","plt.show()\n","\"\"\""],"execution_count":null,"outputs":[]}]}